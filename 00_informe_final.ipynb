# Introducción 
...

# Antecedentes
...

# Objetivos y Metricas de Evaluación
...

# Planteamiento del problema
...

# Desarrollo de la solución
### EDA

##### Metodologia

Se hizo un analisis exploratorio sistematico relacionado a los roles de los jugadores; a de manera biyectiva, cuales son los puntos que pueden llegar a lograr. 
De esa manera medimos lo siguiente (Por rol):
* La distribución de puntos que usualmente generan
* Su distribucion de yardas recorridas o pases completados
* Su precision en recepciones, intercepciones o patadas y la distancia de estos
* Una comparacion de las anteriores estadisticas con la cantidad de puntos 

##### Hallazos Clave
<div align="center">
  <img src="imgs/plots/quaterback%20analytic.png" width="700"/>
</div>
...

<div align="center">
  <img src="imgs/plots/kicker%20analytic.png" width="700"/>
</div>
...

<div align="center">
  <img src="imgs/plots/wide%20receiver%20analytic.png" width="700"/>
</div>
...

<div align="center">
  <img src="imgs/plots/running%20back%20analytic.png" width="700"/>
</div>
...

<div align="center">
  <img src="imgs/plots/tight%20end%20analytic.png" width="700"/>
</div>
...

<div align="center">
  <img src="imgs/plots/defense%20analytic.png" width="700"/>
</div>
...

<div align="center">
  <img src="imgs/plots/pst%20distro.png" width="700"/>
</div>
...

##### Variables cantidatas a ingenieria de caracteristicas
Todas las variables categoricas son cantidatas a un OHE para su procesamiento.

##### Riesgos detectados
Algunas columnas pueden provocar data leakage ya que son reflejos de la variable objetivo "FTPS".

### Data Wrangling
Lista de transformaciones:
* Etiquetado de posición - Se añade la columna Position a cada dataframe individual(DST, K, QB, RB, TE, WR). Justificacion: Necesitaremos reconocer posiciones cuando juntemos todos los df's para procesarlos. Alternativas consideradas: Procesar cada df individualmente pero debido al aumento de complejidad, descartado.
* Unión de dataset - Concatenación de todos los dataframes en un solo dataset general. Justificación: Para hacer un procesamiento formal de los datos decisimos juntar a todos los jugadores de la liga en el mismo df. Alternativas consideradas: NA.
* Limpieza inicial - Eliminación de columnas irrelevantes: Player, Team, FPTS, FPTS/G. Justificación: estas son las col que podian crear data leakage + col que no tienen efecto directo en los datos. Alternativa considerada: NA.
* Conversión de tipos - Justificacion: en el caso que hubiera str que realmente eran int se decidio volver a transformar col con >50% en datos num. a num. solo para asegurar la limpieza de los datos. De misma manera pudimos clasificar los que no eran numericos. Alternativa considerada: NA. 
* Imputación de valores faltantes. Justificación: decidimos utilizar la mediana en vez de la media ya que en el deporte puede haber deportistas estrella o outliers practicamente por lo que la mediana nos servira para balancear ese fenomeno. Alternativa considerada: Utilizar la media pero fue descartada. 
* Codificación categórica OHE. Justificación: Es la manera mas directa de poder computar datos categoricos, fue utilizada por la complejidad de los datos y su buen rendimiento. Alternativa considerada: Mineria de Datos. 

Evidencias (Antes/Despues):
* Antes:

| Rank | Player                      | SACK | INT | FR | FF | DEF TD | SFTY | SPC TD | G  | ... | TD  | SACKS | ATT.1 | YDS.1 | TD.1 | FL  | 20+ | TGT | REC | Y/R |
|------|-----------------------------|------|-----|----|----|--------|------|--------|----|-----|-----|-------|--------|--------|------|-----|-----|-----|-----|-----|
| 1.0  | Houston Texans (HOU)        | 33.0 | 12.0| 7.0| 8.0| 2.0    | 0.0  | 0.0    | 11.0| ... | NaN | NaN   | NaN    | NaN    | NaN  | NaN | NaN | NaN | NaN | NaN |
| 2.0  | Los Angeles Rams (LAR)      | 31.0 | 12.0| 7.0|10.0| 1.0    | 0.0  | 0.0    | 11.0| ... | NaN | NaN   | NaN    | NaN    | NaN  | NaN | NaN | NaN | NaN | NaN |
| 3.0  | Seattle Seahawks (SEA)      | 36.0 | 9.0 | 4.0| 3.0| 2.0    | 0.0  | 2.0    | 11.0| ... | NaN | NaN   | NaN    | NaN    | NaN  | NaN | NaN | NaN | NaN | NaN |
| 4.0  | Cleveland Browns (CLE)      | 42.0 | 9.0 | 6.0|10.0| 2.0    | 0.0  | 0.0    | 11.0| ... | NaN | NaN   | NaN    | NaN    | NaN  | NaN | NaN | NaN | NaN | NaN |
| 5.0  | Pittsburgh Steelers (PIT)   | 34.0 | 9.0 |11.0|13.0| 3.0    | 0.0  | 0.0    | 11.0| ... | NaN | NaN   | NaN    | NaN    | NaN  | NaN | NaN | NaN | NaN | NaN |

* Despues: 

| Rank | SACK | INT | FR  | FF  | DEF TD | SFTY | SPC TD | G   | FG  | ... | ROST_99.4% | ROST_99.6% | ROST_99.7% | ROST_99.8% | ROST_99.9% | Position_K | Position_QB | Position_RB | Position_TE | Position_WR |
|------|------|-----|-----|-----|--------|------|--------|-----|-----|-----|------------|------------|------------|------------|------------|-------------|--------------|--------------|--------------|--------------|
| 1.0  | 33.0 |12.0 | 7.0 | 8.0 | 2.0    | 0.0  | 0.0    |11.0 | 9.5 | ... | False      | False      | False      | False      | False      | False       | False        | False        | False        | False        |
| 2.0  | 31.0 |12.0 | 7.0 |10.0 | 1.0    | 0.0  | 0.0    |11.0 | 9.5 | ... | False      | False      | False      | False      | False      | False       | False        | False        | False        | False        |
| 3.0  | 36.0 | 9.0 | 4.0 | 3.0 | 2.0    | 0.0  | 2.0    |11.0 | 9.5 | ... | False      | False      | False      | False      | False      | False       | False        | False        | False        | False        |
| 4.0  | 42.0 | 9.0 | 6.0 |10.0 | 2.0    | 0.0  | 0.0    |11.0 | 9.5 | ... | False      | False      | False      | False      | False      | False       | False        | False        | False        | False        |
| 5.0  | 34.0 | 9.0 |11.0 |13.0 | 3.0    | 0.0  | 0.0    |11.0 | 9.5 | ... | False      | False      | False      | False      | False      | False       | False        | False        | False        | False        |

Resultado final(df.info):

<class 'pandas.core.frame.DataFrame'>
RangeIndex: 983 entries, 0 to 982
Columns: 317 entries, Rank to Position_WR
dtypes: bool(283), float64(34)
memory usage: 532.9 KB
None


Ruta final del Dataset:
* Antes:
"DraftFantasy/src/data/df.csv"

* Despues 
"DraftFantasy/src/data/df_processed_eg.csv"

Referencia tecnica completa:
"DraftFantasy/src/notebooks/02_DataWrangling.ipynb"

### Entrenmamiento con MLFLOw

Familia de modelos utilizada: 
* XGBoost: con el siguiente espacio de busqueda {
                "n_estimators": trial.suggest_int("n_estimators", 100, 500),
                "max_depth": trial.suggest_int("max_depth", 3, 15),
                "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3),
                "subsample": trial.suggest_float("subsample", 0.5, 1.0),
                "colsample_bytree": trial.suggest_float("colsample_bytree", 0.5, 1.0),
                "reg_lambda": trial.suggest_float("reg_lambda", 1e-3, 10, log=True),
                "reg_alpha": trial.suggest_float("reg_alpha", 1e-3, 10, log=True)
            }

* LightGBM: con el siguiente espacio de busqueda {
                "n_estimators": trial.suggest_int("n_estimators", 100, 500),
                "num_leaves": trial.suggest_int("num_leaves", 20, 120),
                "max_depth": trial.suggest_int("max_depth", -1, 20),
                "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3),
                "subsample": trial.suggest_float("subsample", 0.5, 1.0),
                "colsample_bytree": trial.suggest_float("colsample_bytree", 0.5, 1.0),
                "reg_lambda": trial.suggest_float("reg_lambda", 1e-3, 10, log=True),
                "reg_alpha": trial.suggest_float("reg_alpha", 1e-3, 10, log=True)
            }

* Random Forest: con el siguiente espacio de busqueda {
                "n_estimators": trial.suggest_int("n_estimators", 100, 500),
                "max_depth": trial.suggest_int("max_depth", 3, 40),
                "min_samples_split": trial.suggest_int("min_samples_split", 2, 20),
                "min_samples_leaf": trial.suggest_int("min_samples_leaf", 1, 10),
                "max_features": trial.suggest_categorical("max_features", ["sqrt", "log2", None]),
            }


Registro de MlFlow:
La estrutura de los experimentos fue la siguiente, se hizo la busqueda de hiperparametros, 50 trials alternando entre los 3 modelos de esos 50 trials, se guarda el modelo con el mejor espacio de bisqueda que registro un menor rmse. Se registraron, el rmse, el r2 y el mae. Pero rmse fue la metrica seleccionada como criterio de uso. El mejor modelo se creo y se registro. 

Optimizacion de Hiperparametros:
Se utilizo la libreria optuna, con un numero de evaluaciones de 50 trials. El espacio de busqueda ya fue descrito anteriormente.

Evidencias(Mejores experimentos):
<div align="center">
  <img src="imgs/ss/mlflow/best_experiments.PNG" width="700"/>
</div>

Artifacts registrados:
Los artifacts que se crearon fue principalmente el preprocesador se puede encontrar en "DraftFantasy/src/pipelines/artifacts" como preprocessor.pkl

### Selección del mejor modelo. 

Criterio Objetivo:

Como se puede adelantar en nuestra anterior sección se selecciono el mejor modelo por medio del rmse minimizandolo.

Comparación consolidada: 

| Run | Model    | n_estimators | max_depth | learning_rate | subsample | colsample_bytree | reg_lambda | reg_alpha | num_leaves |   RMSE   |   MAE   |    R²    |
|-----|----------|--------------|-----------|----------------|-----------|-------------------|------------|-----------|-------------|----------|---------|----------|
| 59  | xgboost  | 127          | 13        | 0.0978         | 0.5102    | 0.6810            | 0.0203     | 0.9403    | None        | 5.959179 | 2.609105| 0.979942 |
| 111 | xgboost  | 283          | 4         | 0.0923         | 0.7553    | 0.5388            | 1.5395     | 1.1419    | None        | 6.365348 | 2.679624| 0.977115 |
| 163 | xgboost  | 378          | 8         | 0.0943         | 0.5017    | 0.6147            | 0.1431     | 0.1255    | None        | 6.421486 | 2.498007| 0.976709 |
| 215 | lightgbm | 445          | 9         | 0.0594         | 0.6994    | 0.6917            | 0.2016     | 5.5862    | 97          | 6.550088 | 2.903093| 0.975767 |
| 0   | lightgbm | 362          | 9         | 0.0617         | 0.7830    | 0.9940            | 0.0072     | 0.2217    | 56          | 6.657549 | 2.998966| 0.974965 |

Justificacion final: Los modelos presentados aqui fueron los modelos registrados, es decir. Lo mejor del espacio de busqueda para cada experimento. Aqui algo que modelos destacar es lo pequeño que es reg_lamada y lo bien que optimiza lo que sugiere que los cambios son por detalles menores en el juego.

Registro del Modelo Ganador:

El modelo ganador siempre es registrado como champion.

### Orquestación

Descripción del flujo automatizado:
1. La data se pasa a la funcion de preprocesamiento que genera un artefacto llamado preprocessor.pkl que se usara para el uso en la API
2. Los datos tratados se pasan a los optimizadores del espacio de busqueda.
3. El modelo con los mejores hiperparametros es creado y evaluado.
4. Esta información es guardada en el unity_catalog de Databricks.

Comandos:

python.exe DraftFantasy/src/pipelines/train_pipeline.py

Manejo de fallos:

Excepciones Manejadas con ValueError()

Evidencias:

<div align="center">
  <img src="imgs/diagrams/train_pipeline_diagram.png" width="700"/>
  <br>
  <img src="imgs/ss/misc/train_flow_running.PNG" width="700"/>
  <br>
  <img src="imgs/ss/prefect/prefect_history.PNG" width="700"/>
  <br>
  <img src="imgs/ss/prefect/prefect_flow.PNG" width="700"/>
</div>

### Servir modelo (API con FastAPI)

Objetivo del servicio: 

El objetivo del servicio es servir la mejor prospeccion de un jugador la proxima semana, asi otorgar un buen draft al jugador.

Endpoints:

* (/) root
* (/health) health check
* (/predict) predecir mejor jugador
* (/predict_batch) predecir los mejores 10 jugadores

Flujo del modelo:

El flujo del modelo se puede apreciar en el siguiente diagrama
<div align="center">
  <img src="imgs/diagrams/deploy_diagram.png" width="700"/>
</div>

Podemos ver que la API recibe la informacion ya preprocesada, llega una request (que se manejan a traves del sistema 200 success, 500 error) y colecta el modelo del model registry con anterioridad y ejecuta el algoritmo para mandar la respuesta a la UI.

Referencia de codigo:

La API se encuentra en "DraftFantasy/src/backend/api.py"

Evidencia:

<div align="center">
  <img src="imgs/ss/api/API_ss_aws.PNG" width="700"/>
</div>

### Interfaz Gráfica (Streamlit)

Objetivo:
* Brindar al usuario el servicio de asistencia por ML para su Draft. 
* Brindar al servicio de otros servicios cuantitativos tradicionales, como mejor alineacion o calculo de valor en el trade. 

Componentes minimos:
Iniciar tu session de Draft. Cada vez que pasen las rondas el modelo encontrara el mejor jugador disponible o diferentes opciones igual de buenas (batch).

Indicaciones de ejecucion local:

streamlit run DraftFantasy/src/frontend/app.py

La UI se encuentra en "DraftFantasy/src/frontend/app.py"


<div align="center">
  <video width="700" controls>
    <source src="imgs/ss/GUI/streamlit_aws.mp4" type="video/mp4">
  </video>
</div>


### Contenerización del servicio

Dockerfile backend:
```
FROM python:3.11-slim

WORKDIR /app

COPY backend/api.py ./api.py

COPY backend/requirements.txt ./requirements.txt
RUN pip install --no-cache-dir -r requirements.txt

EXPOSE 8000

CMD ["uvicorn", "api:app", "--host", "0.0.0.0", "--port", "8000"]


```


Dockerfile frontend:
```
FROM python:3.11-slim

WORKDIR /app

COPY frontend/app.py ./app.py

COPY frontend/requirements.txt ./requirements.txt
RUN pip install --no-cache-dir -r requirements.txt

EXPOSE 8501

CMD ["streamlit", "run", "app.py", "--server.address=0.0.0.0", "--server.port=8501"]


```

docker-compose.yml

```
version: "3.8"

services:
  backend:
    build:
      context: .
      dockerfile: backend/Dockerfile
    ports:
      - "8000:8000"
    environment:
      - DATABRICKS_HOST=${DATABRICKS_HOST}
      - DATABRICKS_TOKEN=${DATABRICKS_TOKEN}
    volumes:
      - ./pipelines/artifacts/preprocessors:/app/preprocessors:ro
    restart: unless-stopped

  frontend:
    build:
      context: .
      dockerfile: frontend/Dockerfile
    ports:
      - "8501:8501"
    depends_on:
      - backend
    environment:
      - API_URL=http://backend:8000
    volumes:
      - ./data:/app/data:ro
    restart: unless-stopped
```



<div align="center">
  <img src="imgs/diagrams/docker_diagram.png" width="700"/>
</div>

### Despliegue del servicio en la nube

Plataforma elegida: 

AWS

Pasos:

1. Iniciar una instancia de EC2 (Liberar puerto 8000 y 8501, apartar al menos 15 gib, OS: ubuntu)
2. Ingresar a la computadora a traves de tu llave .pem previamente descargada o traves de la consola de aws.
3. git clone (a este repositorio)
4. Descargar docker-compose

Para descargar docker-compose ejecutar los siguientes comandos en orden.

* sudo apt update
* sudo apt upgrade -y
* sudo apt install -y apt-transport-https ca-certificates curl software-properties-common
* curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
* sudo apt update
* sudo apt install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin
* sudo systemctl start docker
* sudo systemctl enable docker
* sudo usermod -aG docker $USER
* exit

Despues vuelves a acceder a la maquina, eso significa que se ha reiniciado.

5. Acceder a DraftFantasy/src
6. Crear tu archivo .env

Para crear tu archivo .env sigue las siguientes instrucciones y comandos

* nano .env
* escribir DATABRICKS_HOST=tu_url /parse DATABRICKS_TOKEN=tu_token
* ctrl + x
* Guardar

7. Ejecutar docker-compose up --build
8. Esperar a que los contenedores se levantes
9. Acceder a tus puertos a traves de la ip publica de AWS


Evidencia:

<div align="center">
  <img src="imgs/ss/docker/ready_docker_aws.PNG" width="700"/>
</div>

Limpieza/Costos:

para detener los contenedores ejecuta el siguiente comando: 

* docker-compose down

Ahora para limpiar el cache y el almacenamiento ejecuta los siguientes pasos:


* docker stop $(docker ps -aq) # Esto deberia ya estar hecho, por el anterior comando

* docker rm $(docker ps -aq) # Elimina contenedores

* docker rmi $(docker images -q) -f # Elimina imagenes

* docker volume rm $(docker volume ls -q) # elimina voluimenes

* docker network prune -f # elimina redes 

* docker system df # verificar recursos usados


# Conclusiones y recomendaciones
...

# Referencias
* https://docs.docker.com/
* https://docs.astral.sh/uv/guides/package/
* https://docs.databricks.com/aws/en/
* https://fastapi.tiangolo.com/
* https://www.prefect.io/
* https://streamlit.io/
